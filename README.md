# ğŸ› ï¸ Synthetic Data Forge

A powerful Streamlit-based platform for generating realistic, privacy-safe synthetic datasets. Upload sample files, define schemas, inject business logic via LLM, generate multi-table relational data, simulate temporal trends, and push output to local storage or S3 â€” all through an intuitive tabbed web UI.

---

## âœ¨ Features

### ğŸ“Š Single Table Generation
- **Schema Inference** â€” Upload CSV or Parquet files to auto-detect column types
- **Interactive Schema Editor** â€” Modify types (`Int64`, `Float64`, `String`, `Date`) before generation
- **Output Format Selection** â€” Export as **Parquet**, **CSV**, or **JSON**
- **Hive-Style Partitioning** â€” Nest output by multiple partition columns (e.g., `region=US/year=2024/part_0.parquet`)
- **Scalable Generation** â€” Produce thousands of realistic records using [Faker](https://faker.readthedocs.io/)

### ğŸ§  LLM-Powered Business Logic Injection
- Write natural language rules like *"discount_price must be less than original_price"*
- Rules are translated into Python filters via a local [Ollama](https://ollama.ai/) LLM (runs in Docker)
- Generated data is automatically filtered to satisfy all constraints
- Graceful degradation when Ollama is unavailable

### ğŸ›¡ï¸ Privacy Scorecard (DCR Metric)
- Computes **Distance to Closest Record** between real and synthetic datasets
- Flags near-exact matches as potential privacy leaks
- Color-coded risk assessment: ğŸŸ¢ Low / ğŸŸ¡ Medium / ğŸ”´ High
- Distribution histogram and detailed metrics dashboard

### ğŸ”— Multi-Table Relational Integrity (Hydra)
- Upload multiple related files and define **foreign key relationships** via UI
- DAG-based generation order (parents before children) ensures FK consistency
- Mermaid diagram visualization of the relational map
- Per-table row counts and independent schema editing

### â° Time-Travel Simulation
- Generate data across configurable time periods (daily / weekly / monthly)
- **Trend slider** â€” simulate growth or decline (-20% to +20% per period)
- **Spike injection** â€” add date-specific volume multipliers (e.g., Black Friday = 3Ã—)
- Volume preview chart before generation
- Auto-partitioned by time period

### ğŸ“¤ Zero-Copy Cloud Push (Data Sinks)
- **Local Filesystem** â€” write to any local directory with `~/` path expansion
- **Amazon S3** â€” stream data directly from memory to S3 (requires AWS credentials)
- Extensible sink architecture for future targets (Snowflake, BigQuery, Kafka)

---

## ğŸ“‚ Project Structure

```
Synthetic-Data-Forge/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                # Tab-based Streamlit orchestrator
â”‚   â”œâ”€â”€ ui_schema.py           # Reusable schema editor component
â”‚   â”œâ”€â”€ ui_privacy.py          # Privacy scorecard dashboard
â”‚   â”œâ”€â”€ ui_relational.py       # Multi-table relational map UI
â”‚   â””â”€â”€ ui_time_travel.py      # Time-travel trend config UI
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ generator.py           # ForgeEngine â€” synthetic record generation
â”‚   â”œâ”€â”€ llm_logic.py           # LLM business logic injection (Ollama)
â”‚   â”œâ”€â”€ privacy.py             # DCR metric computation
â”‚   â”œâ”€â”€ relational.py          # DAG-based multi-table generation
â”‚   â”œâ”€â”€ time_travel.py         # Temporal trend/spike simulation
â”‚   â””â”€â”€ sinks.py               # Output sinks (Local + S3)
â”œâ”€â”€ .streamlit/
â”‚   â””â”€â”€ config.toml            # Streamlit server config
â”œâ”€â”€ docker-compose.yml         # Ollama LLM server (Docker)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸš€ Getting Started

### Prerequisites

- **Python 3.10+**
- **Docker** (for LLM business logic feature)

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/your-username/Synthetic-Data-Forge.git
   cd Synthetic-Data-Forge
   ```

2. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Start Ollama (for LLM features)**
   ```bash
   docker compose up -d
   docker exec forge-ollama ollama pull llama3
   ```

4. **Run the app**
   ```bash
   python3 -m streamlit run app/main.py
   ```

5. Open **http://localhost:8501**

---

## ğŸ“– Usage

### Single Table Generation
1. Upload a CSV/Parquet sample file
2. Review and edit the inferred schema
3. Choose output format (Parquet/CSV/JSON), record count, and partitioning
4. Optionally add LLM business logic rules
5. Select output sink (Local or S3) and click **Generate**

### Multi-Table (Hydra)
1. Upload 2+ related files
2. Define FK relationships (parent column â†’ child column)
3. Set per-table row counts
4. Generate â€” parents are created first, children get valid FK values

### Time-Travel Simulation
1. Upload a sample file for schema
2. Configure date range, frequency, and trend percentage
3. Add volume spikes on specific dates
4. Preview the volume distribution chart
5. Generate temporal data partitioned by period

### Privacy Scorecard
1. Upload the original (real) dataset
2. Upload synthetic data or use the last generated output
3. View DCR metrics, risk level, and distribution histogram

---

## âš™ï¸ Configuration

### Streamlit (`.streamlit/config.toml`)
```toml
[server]
enableXsrfProtection = false
enableCORS = false
maxUploadSize = 200
```

### S3 Sink
Set AWS credentials via environment variables:
```bash
export AWS_ACCESS_KEY_ID=your-key
export AWS_SECRET_ACCESS_KEY=your-secret
export AWS_DEFAULT_REGION=us-east-1
```

---

## ğŸ“¦ Dependencies

| Package | Purpose |
|---|---|
| [Streamlit](https://streamlit.io/) | Interactive web UI |
| [Polars](https://pola.rs/) | Fast DataFrame operations |
| [Faker](https://faker.readthedocs.io/) | Realistic synthetic data generation |
| [PyArrow](https://arrow.apache.org/docs/python/) | Parquet file I/O |
| [NumPy](https://numpy.org/) + [SciPy](https://scipy.org/) | DCR distance computation |
| [Requests](https://requests.readthedocs.io/) | Ollama API communication |
| [Boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/) | Amazon S3 integration |

---

## ğŸ“„ License

This project is open source. Feel free to use and modify it as needed.
